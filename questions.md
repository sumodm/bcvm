#### Open Questions
1. Why the aversion to negative weights in activation functions ?
2. How are we ensured to get different feature maps ?
3. What are various activation functions ? What are major similarities and differences ?
4. Why does overlapping of pooling layer improve generalizations ? [Alexnet]
5. Why does weight decay reduce model's training error ? [Alexnet]


### March 6, 2016
1. What were the number of parameters in Alexnet?
2. What is Contrast Normalization in GoogLeNet?
3. Why 1x1 convolutional layer? [M. Lin, Q. Chen, and S. Yan. Network in network. CoRR, abs/1312.4400, 2013]
4. What is the significance of Sparse Deep Neural Network and clustering of units with high correlation? [S. Arora, A. Bhaskara, R. Ge, and T. Ma. Provable boundsforlearningsomedeeprepresentations. CoRR, abs/1310.6343, 2013.]
